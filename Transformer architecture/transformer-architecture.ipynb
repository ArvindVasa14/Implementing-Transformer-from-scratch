{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2024-03-25T13:40:43.527323Z","iopub.status.busy":"2024-03-25T13:40:43.526897Z","iopub.status.idle":"2024-03-25T13:40:51.487855Z","shell.execute_reply":"2024-03-25T13:40:51.486640Z","shell.execute_reply.started":"2024-03-25T13:40:43.527279Z"},"trusted":true},"outputs":[{"data":{"text/plain":["'2.10.0'"]},"execution_count":1,"metadata":{},"output_type":"execute_result"}],"source":["import tensorflow as tf\n","\n","tf.__version__"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2024-03-25T13:41:49.178436Z","iopub.status.busy":"2024-03-25T13:41:49.177994Z","iopub.status.idle":"2024-03-25T13:41:49.184335Z","shell.execute_reply":"2024-03-25T13:41:49.183041Z","shell.execute_reply.started":"2024-03-25T13:41:49.178403Z"},"trusted":true},"outputs":[],"source":["import numpy as np"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2024-03-25T13:41:25.314791Z","iopub.status.busy":"2024-03-25T13:41:25.313585Z","iopub.status.idle":"2024-03-25T13:41:25.322692Z","shell.execute_reply":"2024-03-25T13:41:25.321287Z","shell.execute_reply.started":"2024-03-25T13:41:25.314753Z"},"trusted":true},"outputs":[],"source":["def positional_encoding(length, depth):\n","    depth = depth/2\n","\n","    positions = np.arange(length)[:, np.newaxis]     # (seq, 1)\n","    depths = np.arange(depth)[np.newaxis, :]/depth   # (1, depth)\n","\n","    angle_rates = 1 / (10000**depths)         # (1, depth)\n","    angle_rads = positions * angle_rates      # (pos, depth)\n","\n","    pos_encoding = np.concatenate(\n","      [np.sin(angle_rads), np.cos(angle_rads)],\n","      axis=-1) \n","\n","    return tf.cast(pos_encoding, dtype=tf.float32)"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2024-03-25T13:42:10.918727Z","iopub.status.busy":"2024-03-25T13:42:10.918277Z","iopub.status.idle":"2024-03-25T13:42:10.928944Z","shell.execute_reply":"2024-03-25T13:42:10.927524Z","shell.execute_reply.started":"2024-03-25T13:42:10.918693Z"},"trusted":true},"outputs":[{"data":{"text/plain":["<tf.Tensor: shape=(2, 6), dtype=float32, numpy=\n","array([[0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 1.0000000e+00,\n","        1.0000000e+00, 1.0000000e+00],\n","       [8.4147096e-01, 2.5116222e-02, 6.3095731e-04, 5.4030228e-01,\n","        9.9968451e-01, 9.9999982e-01]], dtype=float32)>"]},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":["positional_encoding(2, 5) # 2 embeddings of dim 5"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2024-03-25T13:47:54.727595Z","iopub.status.busy":"2024-03-25T13:47:54.727140Z","iopub.status.idle":"2024-03-25T13:47:54.738284Z","shell.execute_reply":"2024-03-25T13:47:54.737048Z","shell.execute_reply.started":"2024-03-25T13:47:54.727562Z"},"trusted":true},"outputs":[],"source":["class PositionalEmbedding(tf.keras.layers.Layer):\n","    def __init__(self, vocab_size, d_model):\n","        super().__init__()\n","        self.d_model= d_model\n","        self.vocab_size= vocab_size\n","        self.embedding= tf.keras.layers.Embedding(vocab_size, d_model, mask_zero= True)\n","        self.pos_encoding= positional_encoding(length= 2048, depth= d_model)\n","        \n","    def compute_mask(self, *args, **kwargs):\n","        return self.embedding.compute_mask(*args, **kwargs)\n","    \n","    def call(self, x):\n","        length= tf.shape(x)[1]\n","        x= self.embedding(x)\n","        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n","        x = x + self.pos_encoding[tf.newaxis, :length, :]\n","        return x"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2024-03-25T13:52:12.069183Z","iopub.status.busy":"2024-03-25T13:52:12.067800Z","iopub.status.idle":"2024-03-25T13:52:12.115773Z","shell.execute_reply":"2024-03-25T13:52:12.114395Z","shell.execute_reply.started":"2024-03-25T13:52:12.069119Z"},"trusted":true},"outputs":[{"data":{"text/plain":["<tf.Tensor: shape=(2, 5, 128), dtype=float32, numpy=\n","array([[[-0.37939352, -0.40388265,  0.5624964 , ...,  1.0007817 ,\n","          0.53589094,  1.2288162 ],\n","        [ 0.56170964,  0.89567804,  0.88385195, ...,  1.5125477 ,\n","          0.59909   ,  1.4567634 ],\n","        [ 0.8892498 ,  1.3177016 ,  1.4150374 , ...,  0.5758087 ,\n","          1.3145587 ,  0.7270895 ],\n","        [-0.11647467,  0.14756337,  1.2834562 , ...,  0.8859288 ,\n","          0.6709136 ,  1.2637984 ],\n","        [-1.0143971 , -0.68645775,  0.6467226 , ...,  0.88592875,\n","          0.6709135 ,  1.2637982 ]],\n","\n","       [[ 0.35521013, -0.4288043 , -0.01201109, ...,  1.0538578 ,\n","          1.2682743 ,  0.68189716],\n","        [ 0.46764663,  0.8095582 ,  0.65483797, ...,  1.0071988 ,\n","          1.0201993 ,  1.1092464 ],\n","        [ 0.65170276,  0.61730385,  1.5026636 , ...,  0.88592887,\n","          0.6709136 ,  1.2637985 ],\n","        [-0.11647467,  0.14756337,  1.2834562 , ...,  0.8859288 ,\n","          0.6709136 ,  1.2637984 ],\n","        [-1.0143971 , -0.68645775,  0.6467226 , ...,  0.88592875,\n","          0.6709135 ,  1.2637982 ]]], dtype=float32)>"]},"execution_count":6,"metadata":{},"output_type":"execute_result"}],"source":["pos= PositionalEmbedding(100, 128)\n","example_input = np.array([[1, 2, 3, 0, 0], [4, 5, 0, 0, 0]])\n","pos(example_input)"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2024-03-25T13:56:54.815289Z","iopub.status.busy":"2024-03-25T13:56:54.814859Z","iopub.status.idle":"2024-03-25T13:56:54.823432Z","shell.execute_reply":"2024-03-25T13:56:54.822198Z","shell.execute_reply.started":"2024-03-25T13:56:54.815244Z"},"trusted":true},"outputs":[],"source":["class BaseAttention(tf.keras.layers.Layer):\n","    def __init__(self, **kwargs):\n","        super().__init__()\n","        self.mha= tf.keras.layers.MultiHeadAttention(**kwargs)\n","        self.layernorm= tf.keras.layers.LayerNormalization()\n","        self.add= tf.keras.layers.Add()"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[],"source":["class CrossAttention(BaseAttention):\n","    def call(self, x, context):\n","        attn_output, attn_scores= self.mha(\n","            query= x,\n","            key= context,\n","            value= context,\n","            return_attention_scores= True\n","        )\n","\n","        self.last_attn_scores= attn_scores\n","\n","        x= self.add([x, attn_output])\n","        x= self.layernorm(x)\n","\n","        return x\n","    \n","    # output length will be query length"]},{"cell_type":"code","execution_count":16,"metadata":{},"outputs":[],"source":["class GlobalSelfAttention(BaseAttention):\n","    def call(self, x):\n","        attn_output= self.mha(\n","            query= x,\n","            key= x, \n","            value= x\n","        )\n","\n","        x= self.add([x, attn_output])\n","        x= self.layernorm(x)\n","\n","        return x"]},{"cell_type":"code","execution_count":21,"metadata":{},"outputs":[],"source":["x = tf.random.normal(shape=(32, 10, 64))\n","\n","# Create an instance of GlobalSelfAttention\n","attention_layer = GlobalSelfAttention(num_heads=8, key_dim=512)\n","\n","# Call the attention layer with the input tensor\n","output = attention_layer(x)"]},{"cell_type":"code","execution_count":25,"metadata":{},"outputs":[{"data":{"text/plain":["TensorShape([32, 10, 64])"]},"execution_count":25,"metadata":{},"output_type":"execute_result"}],"source":["x.shape"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["output"]},{"cell_type":"code","execution_count":27,"metadata":{},"outputs":[],"source":["class CausalSelfAttention(BaseAttention):\n","    def call(self, x):\n","        attn_output= self.mha(\n","            query= x,\n","            key= x,\n","            value= x,\n","            use_causal_mask = True\n","        )\n","\n","        x= self.add([x, attn_output])\n","        x= self.layernorm(x)\n","\n","        return x"]},{"cell_type":"code","execution_count":31,"metadata":{},"outputs":[],"source":["sample_ca= CausalSelfAttention(num_heads= 2, key_dim= 512)\n","out3= sample_ca(x)"]},{"cell_type":"code","execution_count":32,"metadata":{},"outputs":[{"data":{"text/plain":["TensorShape([32, 10, 64])"]},"execution_count":32,"metadata":{},"output_type":"execute_result"}],"source":["out3.shape"]},{"cell_type":"code","execution_count":34,"metadata":{},"outputs":[],"source":["class FeedForward(tf.keras.layers.Layer):\n","    def __init__(self, d_model, dff, dropout_rate= 0.1):\n","        super().__init__()\n","        self.seq= tf.keras.Sequential([\n","            tf.keras.layers.Dense(dff, activation= 'relu'),\n","            tf.keras.layers.Dense(d_model),\n","            tf.keras.layers.Dropout(dropout_rate)\n","        ])\n","\n","        self.add= tf.keras.layers.Add()\n","        self.layer_norm= tf.keras.layers.LayerNormalization()\n","\n","    def call(self, x):\n","        x= self.add([x, self.seq(x)])\n","        x= self.layer_norm(x)\n","\n","        return x"]},{"cell_type":"code","execution_count":39,"metadata":{},"outputs":[],"source":["sample_ffn= FeedForward(64, 2048)\n","x = tf.random.normal(shape=(32, 10, 64))\n","out4= sample_ffn(x)"]},{"cell_type":"code","execution_count":40,"metadata":{},"outputs":[{"data":{"text/plain":["TensorShape([32, 10, 64])"]},"execution_count":40,"metadata":{},"output_type":"execute_result"}],"source":["out4.shape"]},{"cell_type":"markdown","metadata":{},"source":["### The encoder layer\n","\n","The encoder contains a stack of N encoder layers. Where each EncoderLayer contains a GlobalSelfAttention and FeedForward layer:"]},{"cell_type":"code","execution_count":43,"metadata":{},"outputs":[],"source":["class EncoderLayer(tf.keras.layers.Layer):\n","    def __init__(self,*, d_model, num_heads, dff, dropout_rate= 0.1):\n","        super().__init__()\n","\n","        # Attention layer\n","        self.self_attention= GlobalSelfAttention(\n","            num_heads= num_heads,\n","            key_dim= d_model,\n","            dropout= dropout_rate\n","        )\n","\n","        self.fnn= FeedForward(d_model, dff)\n","\n","    def call(self, x):\n","        x= self.self_attention(x)\n","        x= self.fnn(x)\n","\n","        return x"]},{"cell_type":"code","execution_count":45,"metadata":{},"outputs":[],"source":["sample_encoder_layer = EncoderLayer(d_model=64, num_heads=8, dff=2048)\n","x = tf.random.normal(shape=(32, 10, 64))\n","out5= sample_encoder_layer(x)"]},{"cell_type":"code","execution_count":47,"metadata":{},"outputs":[{"data":{"text/plain":["TensorShape([32, 10, 64])"]},"execution_count":47,"metadata":{},"output_type":"execute_result"}],"source":["out5.shape"]},{"cell_type":"code","execution_count":48,"metadata":{},"outputs":[],"source":["class Encoder(tf.keras.layers.Layer):\n","    def __init__(self, *, num_layers, d_model, num_heads, dff, vocab_size, dropout_rate= 0.1):\n","        super().__init__()\n","        self.d_model= d_model\n","        self.num_layers= num_layers\n","\n","        self.pos_embedding= PositionalEmbedding(\n","            vocab_size= vocab_size, d_model= d_model\n","        )\n","\n","        self.enc_layers= [ \n","            EncoderLayer(d_model= d_model, num_heads= num_heads, dff= dff, dropout_rate= dropout_rate)\n","            for _ in range(num_layers)\n","        ]\n","\n","        self.dropout= tf.keras.layers.Dropout(dropout_rate)\n","\n","    def call(self, x):\n","        x= self.pos_embedding(x)\n","        x= self.dropout(x)\n","        \n","        for i in range(self.num_layers):\n","            x= self.enc_layers[i](x)\n","        \n","        return x"]},{"cell_type":"code","execution_count":51,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Encoder output shape: (32, 20, 512)\n"]}],"source":["import tensorflow as tf\n","\n","# Example input parameters\n","batch_size = 32\n","seq_length = 20\n","vocab_size = 10000  # Example vocabulary size\n","d_model = 512  # Example dimensionality of the model\n","num_layers = 6  # Example number of layers in the encoder\n","num_heads = 8  # Example number of attention heads\n","dff = 2048  # Example dimensionality of the feedforward layer\n","dropout_rate = 0.1  # Example dropout rate\n","\n","# Create a random batch of sequences\n","input_sequences = tf.random.uniform(shape=(batch_size, seq_length), minval=0, maxval=vocab_size, dtype=tf.int32)\n","\n","# Create an instance of the Encoder class\n","encoder = Encoder(num_layers=num_layers, d_model=d_model, num_heads=num_heads, dff=dff, vocab_size=vocab_size, dropout_rate=dropout_rate)\n","\n","# Call the Encoder with the input sequences\n","encoder_output = encoder(input_sequences)\n","\n","# Print the shape of the encoder output\n","print(\"Encoder output shape:\", encoder_output.shape)\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["encoder_output"]},{"cell_type":"code","execution_count":53,"metadata":{},"outputs":[],"source":["class DecoderLayer(tf.keras.layers.Layer):\n","    def __init__(self, *, d_model, num_heads, dff, dropout_rate= 0.1):\n","        super(DecoderLayer, self).__init__()\n","\n","        self.causal_self_attention= CausalSelfAttention(num_heads= num_heads, key_dim= d_model, dropout= dropout_rate)\n","\n","        self.cross_attention= CrossAttention(num_heads= num_heads, key_dim= d_model, dropout= dropout_rate)\n","\n","        self.fnn= FeedForward(d_model, dff)\n","\n","    def call(self, x, context):\n","        x= self.causal_self_attention(x= x)\n","        x= self.cross_attention(x=x, context= context)\n","\n","        self.last_attn_scores= self.cross_attention.last_attn_scores\n","\n","        x= self.fnn(x)\n","\n","        return x\n"]},{"cell_type":"code","execution_count":54,"metadata":{},"outputs":[],"source":["class Decoder(tf.keras.layers.Layer):\n","    def __init__(self, *, num_layers, d_model, num_heads, dff, vocab_size, dropout_rate= 0.1):\n","        super(Decoder, self).__init__()\n","\n","        self.d_model= d_model\n","        self.num_layers= num_layers\n","\n","        self.pos_embedding= PositionalEmbedding(vocab_size=vocab_size, d_model=d_model)\n","\n","        self.dropout= tf.keras.layers.Dropout(dropout_rate)\n","\n","        self.dec_layers= [\n","            DecoderLayer(d_model= d_model, num_heads= num_heads, dff= dff, dropout_rate= dropout_rate)\n","            for _ in range(num_layers)\n","        ]\n","\n","        self.last_attn_scores= None\n","\n","    def call(self, x, context):\n","        x= self.pos_embedding(x)\n","\n","        x= self.dropout(x)\n","\n","        for i in range(self.num_layers):\n","            x= self.dec_layers[i](x, context)\n","\n","        self.last_attn_scores= self.dec_layers[-1].last_attn_scores\n","\n","        return x"]},{"cell_type":"code","execution_count":55,"metadata":{},"outputs":[],"source":["class Transformer(tf.keras.Model):\n","    def __init__(self, *, num_layers, d_model, num_heads, dff, input_vocab_size, target_vocab_size, dropout_rate=0.1):\n","        super().__init__()\n","        self.encoder = Encoder(num_layers=num_layers, d_model=d_model,\n","                            num_heads=num_heads, dff=dff,\n","                            vocab_size=input_vocab_size,\n","                            dropout_rate=dropout_rate)\n","\n","        self.decoder = Decoder(num_layers=num_layers, d_model=d_model,\n","                            num_heads=num_heads, dff=dff,\n","                            vocab_size=target_vocab_size,\n","                            dropout_rate=dropout_rate)\n","        \n","        self.final_layer= tf.keras.layers.Dense(target_vocab_size)\n","\n","    def call(self, inputs):\n","        context, x= inputs\n","        context= self.encoder(context)\n","        x= self.decoder(x, context)\n","        logits= self.final_layer(x)\n","        try:\n","            del logits._keras_mask\n","        except AttributeError:\n","            pass\n","\n","        return logits\n"]},{"cell_type":"code","execution_count":56,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Logits shape: (32, 10, 8000)\n"]}],"source":["import tensorflow as tf\n","\n","# Define example input parameters\n","batch_size = 32\n","context_len = 20\n","target_len = 10\n","input_vocab_size = 10000  # Example input vocabulary size\n","target_vocab_size = 8000  # Example target vocabulary size\n","d_model = 512  # Example dimensionality of the model\n","num_layers = 6  # Example number of layers in the encoder and decoder\n","num_heads = 8  # Example number of attention heads\n","dff = 2048  # Example dimensionality of the feedforward layer\n","dropout_rate = 0.1  # Example dropout rate\n","\n","# Create example input tensors\n","context_input = tf.random.uniform(shape=(batch_size, context_len), minval=0, maxval=input_vocab_size, dtype=tf.int32)\n","target_input = tf.random.uniform(shape=(batch_size, target_len), minval=0, maxval=target_vocab_size, dtype=tf.int32)\n","\n","# Create an instance of the Transformer model\n","transformer_model = Transformer(num_layers=num_layers, d_model=d_model, num_heads=num_heads,\n","                                dff=dff, input_vocab_size=input_vocab_size,\n","                                target_vocab_size=target_vocab_size, dropout_rate=dropout_rate)\n","\n","# Call the Transformer model with the input tensors\n","logits = transformer_model((context_input, target_input))\n","\n","# Print the shape of the logits\n","print(\"Logits shape:\", logits.shape)\n"]},{"cell_type":"code","execution_count":57,"metadata":{},"outputs":[{"data":{"text/plain":["<tf.Tensor: shape=(32, 10, 8000), dtype=float32, numpy=\n","array([[[ 8.41520652e-02,  2.92397216e-02,  5.18674016e-01, ...,\n","          1.52535826e-01,  1.02633379e-01,  3.63305628e-01],\n","        [ 1.76194888e-02, -1.13686271e-01,  6.17882371e-01, ...,\n","          1.80696160e-01, -2.19565853e-01, -4.77595342e-04],\n","        [-6.94560528e-01,  2.60134041e-01,  3.11307967e-01, ...,\n","          2.75901377e-01,  2.05914587e-01, -1.62046179e-01],\n","        ...,\n","        [-3.16444814e-01,  1.37907758e-01,  1.55345887e-01, ...,\n","          1.69166118e-01,  9.86725790e-04,  4.04311746e-01],\n","        [-2.98415542e-01, -5.99588314e-03,  3.34141366e-02, ...,\n","         -1.59954831e-01, -2.80424118e-01,  3.57763052e-01],\n","        [-4.37622964e-01,  2.85261869e-01,  3.31564933e-01, ...,\n","          1.12829329e-02, -6.89318776e-02,  3.78182709e-01]],\n","\n","       [[ 4.77251597e-02,  5.93714528e-02,  3.74049842e-01, ...,\n","          2.54449308e-01,  1.70889527e-01,  4.49835777e-01],\n","        [ 2.19382450e-01,  2.37897485e-02,  2.60597497e-01, ...,\n","          1.23598240e-01, -2.20796481e-01,  6.05844975e-01],\n","        [-7.36875087e-02,  9.18216109e-02,  4.28821325e-01, ...,\n","          3.73840600e-01, -6.08430207e-02,  3.80855054e-01],\n","        ...,\n","        [-3.53345960e-01,  1.77975208e-01,  2.77629346e-01, ...,\n","         -2.03584552e-01, -3.95004526e-02, -2.16770113e-01],\n","        [-3.38620842e-01,  7.03232884e-01,  3.82776380e-01, ...,\n","         -1.73144639e-01,  4.94852871e-01,  1.48787022e-01],\n","        [ 3.52419227e-01,  3.45746547e-01,  3.75180185e-01, ...,\n","         -5.16511761e-02,  2.08992735e-01,  7.96661854e-01]],\n","\n","       [[-2.18048498e-01,  2.11066334e-03,  4.56952631e-01, ...,\n","          6.07015043e-02,  3.58358137e-02,  3.68028462e-01],\n","        [-1.17023043e-01,  2.66872108e-01,  8.60699177e-01, ...,\n","          1.75989211e-01,  9.41659510e-02,  2.77134627e-01],\n","        [-1.79381687e-02,  4.12135899e-01,  2.17257485e-01, ...,\n","          1.39022633e-01,  2.04556108e-01,  2.43530735e-01],\n","        ...,\n","        [-1.82869717e-01,  1.32126257e-01,  1.46818519e-01, ...,\n","         -1.20495148e-01,  7.40578547e-02,  1.75895691e-01],\n","        [-2.59015918e-01, -2.25278452e-01,  6.78575814e-01, ...,\n","         -3.17021132e-01,  2.28076708e-02,  3.75429869e-01],\n","        [-2.82005191e-01,  2.76928157e-01,  4.70077664e-01, ...,\n","         -1.62312835e-01,  4.89983380e-01,  4.91232008e-01]],\n","\n","       ...,\n","\n","       [[-5.53040028e-01,  1.95626304e-01,  2.88688093e-01, ...,\n","          1.04468189e-01,  3.60905170e-01,  1.02878869e-01],\n","        [ 4.90874015e-02,  2.18892857e-01,  6.23418212e-01, ...,\n","         -9.68677178e-02,  1.74449697e-01,  2.55531967e-01],\n","        [-5.23510799e-02,  2.36589730e-01,  2.80662179e-01, ...,\n","          3.41891646e-01,  9.03610289e-02,  4.69847798e-01],\n","        ...,\n","        [-1.82137728e-01, -3.56608741e-02,  5.71321212e-02, ...,\n","         -4.89270687e-02,  1.34955421e-01,  5.71351886e-01],\n","        [-2.45178372e-01, -2.25102082e-01,  6.77977026e-01, ...,\n","         -3.25408101e-01,  6.01902120e-02,  3.88175249e-01],\n","        [ 3.25021863e-01, -1.78387746e-01,  3.08914989e-01, ...,\n","          3.51469442e-02,  6.74689054e-01,  3.22814107e-01]],\n","\n","       [[-3.26221079e-01, -1.27509519e-01,  4.20173138e-01, ...,\n","         -4.52585705e-03,  2.26615831e-01, -7.35401139e-02],\n","        [-4.50327873e-01,  1.34249985e-01,  5.09776138e-02, ...,\n","          6.36308730e-01,  2.92596251e-01,  3.25602055e-01],\n","        [-4.87290919e-01,  1.57126576e-01,  3.70970398e-01, ...,\n","          1.70674339e-01,  4.52284157e-01,  1.10324167e-01],\n","        ...,\n","        [-2.55394816e-01,  9.86380130e-02, -1.02475315e-01, ...,\n","          1.13736741e-01,  4.13982309e-02,  2.40684748e-01],\n","        [-9.19374600e-02,  6.10519759e-02,  2.88758785e-01, ...,\n","         -1.77906737e-01,  3.10189605e-01,  1.78976074e-01],\n","        [ 1.92141626e-02,  5.20009100e-02,  2.23027304e-01, ...,\n","         -1.99366361e-01,  3.15929860e-01,  4.06567037e-01]],\n","\n","       [[ 7.59676099e-02,  5.43809831e-01,  3.67793649e-01, ...,\n","         -1.95800215e-01,  1.47803739e-01,  8.84123296e-02],\n","        [-2.57045507e-01,  3.00707966e-01,  2.40800142e-01, ...,\n","          1.78268194e-01,  2.15987816e-01,  4.86172363e-02],\n","        [-4.88900542e-01,  6.82120770e-02,  5.23991525e-01, ...,\n","          1.83797941e-01,  1.40966609e-01,  1.02095932e-01],\n","        ...,\n","        [-5.12854755e-01,  1.06698796e-01,  3.87989283e-01, ...,\n","          1.06745571e-01, -1.16926529e-01,  2.84339756e-01],\n","        [-5.82828978e-03, -1.40459351e-02,  2.74668723e-01, ...,\n","         -1.93082392e-01,  1.26588985e-01,  3.42038691e-01],\n","        [-2.54611582e-01,  1.30337149e-01,  2.01574579e-01, ...,\n","         -2.92251080e-01,  1.69514135e-01,  3.74631464e-01]]],\n","      dtype=float32)>"]},"execution_count":57,"metadata":{},"output_type":"execute_result"}],"source":["logits"]},{"cell_type":"code","execution_count":58,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Model: \"transformer\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," encoder_3 (Encoder)         multiple                  68139008  \n","                                                                 \n"," decoder (Decoder)           multiple                  117529600 \n","                                                                 \n"," dense_66 (Dense)            multiple                  4104000   \n","                                                                 \n","=================================================================\n","Total params: 189,772,608\n","Trainable params: 189,772,608\n","Non-trainable params: 0\n","_________________________________________________________________\n"]}],"source":["transformer_model.summary()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30673,"isGpuEnabled":false,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.14"}},"nbformat":4,"nbformat_minor":4}
